{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jBe5zsOUnpCf"},"outputs":[],"source":["# ============================================================\n","# Multimodal Image Search Notebook (Colab)\n","# ------------------------------------------------------------\n","# Goal:\n","#   - Download JPG/JPEG/PNG images from S3\n","#   - Create multimodal embeddings with Amazon Bedrock Titan Multimodal Embeddings (G1)\n","#   - Store vectors in ChromaDB\n","#   - Search images with natural language queries (e.g., \"blue t-shirt\")\n","#   - (Optional) Use OpenAI Vision to summarize retrieved images with S1..Sn references\n","#\n","# What you need beforehand:\n","#   - AWS credentials available in Colab (env vars recommended):\n","#       AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION\n","#   - IAM permissions:\n","#       s3:ListBucket, s3:GetObject on your bucket\n","#       bedrock:InvokeModel on the Titan embedding model\n","#   - OpenAI API key (optional for summaries):\n","#       OPENAI_API_KEY\n","# ============================================================\n"]},{"cell_type":"code","source":["# --- Install dependencies (Colab) ---\n","!pip install -q boto3 pillow chromadb openai\n"],"metadata":{"id":"Ov6feTCDnsjn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# 0) Imports + Global Settings\n","# ============================================================\n","import os\n","import io\n","import re\n","import json\n","import base64\n","from pathlib import Path\n","from typing import List, Dict, Tuple, Optional\n","\n","import boto3\n","from botocore.exceptions import ClientError, NoCredentialsError\n","\n","from PIL import Image, ImageOps\n","import matplotlib.pyplot as plt\n","\n","import chromadb\n","from openai import OpenAI\n"],"metadata":{"id":"T1R0vkJ3nt-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ⚠️ DO NOT commit these anywhere permanent\n","os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<YOUR AWS ACCESS KEY>\"\n","os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<YOUR SECRET KEY>\"\n","os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"  # change if needed\n","import os, getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"],"metadata":{"id":"NnWA_JALoci-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# 0A) Configure your run\n","# ============================================================\n","# S3 location\n","BUCKET = \"<YOUR BUCKET NAME>\"\n","PREFIX = \"\"           # e.g., \"photos/2024/\" or \"\" for root\n","SAMPLE_N = 200        # start small; increase as you gain confidence\n","\n","# Bedrock region (must match where you invoke Bedrock)\n","BEDROCK_REGION = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n","\n","# Titan Multimodal Embeddings G1 modelId for embeddings\n","TITAN_MODEL_ID = \"amazon.titan-embed-image-v1\"\n","\n","# Chroma persistence\n","CHROMA_DIR = \"chroma_images_db\"\n","CHROMA_COLLECTION = \"photo_library\"\n","\n","# Embedding dimension (Titan supports 256 / 384 / 1024; 1024 is a strong default)\n","EMBED_DIM = 1024\n","\n","# Local storage for downloaded images\n","LOCAL_IMAGE_DIR = \"data/images\"\n","\n","# Extensions to index from S3\n","IMAGE_EXTS = (\".jpg\", \".jpeg\", \".png\")\n"],"metadata":{"id":"YcfWkWFDnv7E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# 1) S3 Helpers: list + download\n","# ============================================================\n","def get_s3_client(region_name: Optional[str] = None):\n","    \"\"\"\n","    Creates an S3 client. boto3 will automatically use credentials from:\n","      - environment variables\n","      - ~/.aws/credentials (not typical in Colab)\n","      - IAM role (if running on AWS infra)\n","    \"\"\"\n","    return boto3.client(\"s3\", region_name=region_name) if region_name else boto3.client(\"s3\")\n","\n","\n","def list_image_keys(\n","    bucket: str,\n","    prefix: str = \"\",\n","    exts: Tuple[str, ...] = IMAGE_EXTS,\n","    max_keys: Optional[int] = None\n",") -> List[str]:\n","    \"\"\"\n","    List image object keys from S3 under a prefix (paginated).\n","\n","    Usability tips:\n","      - Use prefix to narrow scope (much faster/cheaper).\n","      - Start with max_keys (e.g., 200) to validate before indexing everything.\n","    \"\"\"\n","    s3 = get_s3_client()\n","    keys = []\n","    token = None\n","\n","    try:\n","        while True:\n","            kwargs = {\"Bucket\": bucket, \"Prefix\": prefix}\n","            if token:\n","                kwargs[\"ContinuationToken\"] = token\n","\n","            resp = s3.list_objects_v2(**kwargs)\n","\n","            for obj in resp.get(\"Contents\", []):\n","                key = obj[\"Key\"]\n","                if key.lower().endswith(tuple(e.lower() for e in exts)):\n","                    keys.append(key)\n","                    if max_keys and len(keys) >= max_keys:\n","                        return keys\n","\n","            if resp.get(\"IsTruncated\"):\n","                token = resp.get(\"NextContinuationToken\")\n","            else:\n","                break\n","\n","    except NoCredentialsError:\n","        raise RuntimeError(\"AWS credentials not found. Set env vars or configure auth.\")\n","    except ClientError as e:\n","        raise RuntimeError(f\"S3 list error: {e}\")\n","\n","    return keys\n","\n","\n","def download_s3_objects(bucket: str, keys: List[str], local_dir: str = LOCAL_IMAGE_DIR) -> List[str]:\n","    \"\"\"\n","    Download S3 objects to a local folder.\n","    Recreates S3 'folders' as subfolders locally.\n","\n","    Failure behavior:\n","      - Continues downloading even if some objects fail.\n","      - Prints failures.\n","    \"\"\"\n","    s3 = get_s3_client()\n","    local_dir = Path(local_dir)\n","    local_dir.mkdir(parents=True, exist_ok=True)\n","\n","    downloaded = []\n","    for key in keys:\n","        local_path = local_dir / key\n","        local_path.parent.mkdir(parents=True, exist_ok=True)\n","        try:\n","            s3.download_file(bucket, key, str(local_path))\n","            downloaded.append(str(local_path))\n","        except ClientError as e:\n","            print(f\"[DOWNLOAD FAILED] {key} -> {e}\")\n","\n","    return downloaded\n"],"metadata":{"id":"hJ0p60hhnxIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# 2) Image normalization (recovery from failures)\n","# ------------------------------------------------------------\n","# Why this exists:\n","#   Bedrock may reject some images (\"Unable to process provided image\") due to:\n","#     - corruption/truncation\n","#     - unusual PNG chunks/profiles\n","#     - weird encodings\n","#\n","# We normalize to a clean RGB JPEG and optionally resize.\n","# ============================================================\n","def normalize_image_to_jpeg_bytes(image_path: str, max_side: int = 2048, quality: int = 90) -> bytes:\n","    \"\"\"\n","    Load an image, fix EXIF orientation, convert to RGB, optionally resize,\n","    and return JPEG-encoded bytes.\n","\n","    This dramatically reduces model ingestion failures.\n","    \"\"\"\n","    p = Path(image_path)\n","\n","    with Image.open(p) as img:\n","        # Fix camera rotation from EXIF\n","        img = ImageOps.exif_transpose(img)\n","\n","        # Convert to RGB (drops alpha if present)\n","        if img.mode != \"RGB\":\n","            img = img.convert(\"RGB\")\n","\n","        # Resize if too large (keeps aspect ratio)\n","        w, h = img.size\n","        scale = max(w, h) / max_side\n","        if scale > 1:\n","            new_w = int(w / scale)\n","            new_h = int(h / scale)\n","            img = img.resize((new_w, new_h))\n","\n","        buf = io.BytesIO()\n","        img.save(buf, format=\"JPEG\", quality=quality, optimize=True)\n","        return buf.getvalue()\n"],"metadata":{"id":"deU6NT94nzAV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# 3) Bedrock Titan Multimodal Embeddings (Image + Text)\n","# ============================================================\n","def get_bedrock_runtime(region_name: str = BEDROCK_REGION):\n","    \"\"\"\n","    Bedrock Runtime client used for InvokeModel calls.\n","    \"\"\"\n","    return boto3.client(\"bedrock-runtime\", region_name=region_name)\n","\n","\n","def titan_embed_image_safe(\n","    image_path: str,\n","    output_dim: int = EMBED_DIM,\n","    region: str = BEDROCK_REGION,\n","    model_id: str = TITAN_MODEL_ID\n",") -> List[float]:\n","    \"\"\"\n","    Create an embedding for an image using Titan Multimodal Embeddings G1.\n","\n","    Uses normalization so most problematic images still work.\n","    If an image still fails, we raise with a helpful error.\n","    \"\"\"\n","    br = get_bedrock_runtime(region)\n","\n","    jpeg_bytes = normalize_image_to_jpeg_bytes(image_path)\n","    image_b64 = base64.b64encode(jpeg_bytes).decode(\"utf-8\")\n","\n","    body = json.dumps({\n","        \"inputImage\": image_b64,\n","        \"embeddingConfig\": {\"outputEmbeddingLength\": output_dim}\n","    })\n","\n","    try:\n","        resp = br.invoke_model(\n","            modelId=model_id,\n","            body=body,\n","            accept=\"application/json\",\n","            contentType=\"application/json\"\n","        )\n","        data = json.loads(resp[\"body\"].read())\n","        if data.get(\"message\"):\n","            raise RuntimeError(data[\"message\"])\n","        return data[\"embedding\"]\n","\n","    except ClientError as e:\n","        raise RuntimeError(f\"Bedrock InvokeModel failed: {e}\")\n","\n","\n","def titan_embed_text(\n","    query: str,\n","    output_dim: int = EMBED_DIM,\n","    region: str = BEDROCK_REGION,\n","    model_id: str = TITAN_MODEL_ID\n",") -> List[float]:\n","    \"\"\"\n","    Create an embedding for a text query using the SAME Titan multimodal model.\n","    This is what enables text->image retrieval in one shared vector space.\n","    \"\"\"\n","    br = get_bedrock_runtime(region)\n","\n","    body = json.dumps({\n","        \"inputText\": query,\n","        \"embeddingConfig\": {\"outputEmbeddingLength\": output_dim}\n","    })\n","\n","    try:\n","        resp = br.invoke_model(\n","            modelId=model_id,\n","            body=body,\n","            accept=\"application/json\",\n","            contentType=\"application/json\"\n","        )\n","        data = json.loads(resp[\"body\"].read())\n","        if data.get(\"message\"):\n","            raise RuntimeError(data[\"message\"])\n","        return data[\"embedding\"]\n","\n","    except ClientError as e:\n","        raise RuntimeError(f\"Bedrock InvokeModel failed: {e}\")\n"],"metadata":{"id":"T4ln58PYn0fm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# 4) ChromaDB setup + ingest (upsert) with failure recovery\n","# ============================================================\n","def get_chroma_collection(persist_dir: str = CHROMA_DIR, collection_name: str = CHROMA_COLLECTION):\n","    \"\"\"\n","    Creates/loads a persistent Chroma collection.\n","    - Persistent storage: survives notebook runtime restarts if the directory remains.\n","    - In Colab, runtime resets wipe disk; but within the session it's persisted.\n","    \"\"\"\n","    client = chromadb.PersistentClient(path=persist_dir)\n","    return client.get_or_create_collection(name=collection_name)\n","\n","\n","def build_image_records(image_keys: List[str], local_paths: List[str]) -> List[Dict]:\n","    \"\"\"\n","    Build stable records:\n","      - id: use S3 key (best for re-indexing / updates)\n","      - metadata: includes s3_key and filename\n","      - path: local file path\n","    Assumes lists are aligned (image_keys[i] matches local_paths[i]).\n","    \"\"\"\n","    records = []\n","    for key, path in zip(image_keys, local_paths):\n","        records.append({\n","            \"id\": key,\n","            \"path\": path,\n","            \"metadata\": {\n","                \"s3_key\": key,\n","                \"filename\": Path(path).name\n","            }\n","        })\n","    return records\n","\n","\n","def upsert_images_to_chroma(\n","    collection,\n","    records: List[Dict],\n","    output_dim: int = EMBED_DIM,\n","    batch_size: int = 8,\n","    show_progress_every: int = 10\n",") -> List[Tuple[str, str, str]]:\n","    \"\"\"\n","    Create embeddings for images and upsert them into Chroma.\n","\n","    Failure behavior:\n","      - Skips images that fail embedding\n","      - Keeps going\n","      - Returns a list of (id, path, error) for inspection/retry\n","\n","    Usability tips:\n","      - Start small (SAMPLE_N=200)\n","      - Then scale to thousands once stable\n","    \"\"\"\n","    failed = []\n","    stored_before = collection.count()\n","\n","    for batch_idx, i in enumerate(range(0, len(records), batch_size), start=1):\n","        batch = records[i:i + batch_size]\n","\n","        ids, metas, docs, embs = [], [], [], []\n","\n","        for r in batch:\n","            try:\n","                emb = titan_embed_image_safe(r[\"path\"], output_dim=output_dim)\n","                ids.append(r[\"id\"])\n","                metas.append(r[\"metadata\"])\n","                docs.append(\"\")  # no text doc needed for pure image search\n","                embs.append(emb)\n","            except Exception as e:\n","                failed.append((r[\"id\"], r[\"path\"], str(e)))\n","\n","        # Only upsert if at least one succeeded\n","        if ids:\n","            collection.upsert(ids=ids, metadatas=metas, documents=docs, embeddings=embs)\n","\n","        if (batch_idx % show_progress_every) == 0:\n","            print(f\"[PROGRESS] batches={batch_idx} stored_now={collection.count()} failed={len(failed)}\")\n","\n","    stored_after = collection.count()\n","    print(f\"[DONE] stored_before={stored_before} stored_after={stored_after} newly_added={stored_after - stored_before} failed={len(failed)}\")\n","    return failed\n"],"metadata":{"id":"NCqF1YT6n2FU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# 5) Query (text -> image retrieval) + display results\n","# ============================================================\n","def query_images(collection, text_query: str, k: int = 8, output_dim: int = EMBED_DIM) -> List[Dict]:\n","    \"\"\"\n","    Embed the text query with Titan -> vector search in Chroma -> return top-k hits.\n","    \"\"\"\n","    q_emb = titan_embed_text(text_query, output_dim=output_dim)\n","    res = collection.query(\n","        query_embeddings=[q_emb],\n","        n_results=k,\n","        include=[\"metadatas\", \"distances\"]  # ids always returned by Chroma query\n","    )\n","    return [\n","        {\"id\": _id, \"distance\": dist, \"metadata\": meta}\n","        for _id, dist, meta in zip(res[\"ids\"][0], res[\"distances\"][0], res[\"metadatas\"][0])\n","    ]\n","\n","\n","def show_hit_images(hits: List[Dict], s3_to_local: Dict[str, str], n: int = 6):\n","    \"\"\"\n","    Display top N hits in Colab with their distance and filename.\n","    \"\"\"\n","    for h in hits[:n]:\n","        key = h[\"metadata\"][\"s3_key\"]\n","        path = s3_to_local.get(key)\n","        if not path:\n","            continue\n","\n","        img = Image.open(path)\n","        plt.figure()\n","        plt.imshow(img)\n","        plt.axis(\"off\")\n","        title = f\"{Path(path).name} | dist={h['distance']:.4f}\\n{s3_key_to_display(BUCKET, key)}\"\n","        plt.title(title)\n","        plt.show()\n","\n","\n","def s3_key_to_display(bucket: Optional[str], key: str) -> str:\n","    \"\"\"\n","    Safe display string. Not a public URL (unless your bucket is public).\n","    \"\"\"\n","    return f\"s3://{bucket}/{key}\" if bucket else key\n"],"metadata":{"collapsed":true,"id":"67e559l6n2mo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# 6) (Optional) OpenAI Vision \"G\" step: summarize retrieved images\n","# ------------------------------------------------------------\n","# Goal:\n","#   - Retrieve top-k images (R)\n","#   - Show top N with stable labels S1..Sn\n","#   - Send those same images to OpenAI vision model\n","#   - Model returns summary referencing S1..Sn so you can visually validate\n","#\n","# Requirements:\n","#   - OPENAI_API_KEY set\n","# ============================================================\n","def image_to_b64(path: str) -> str:\n","    return base64.b64encode(Path(path).read_bytes()).decode(\"utf-8\")\n","\n","\n","def display_hit_images_labeled(\n","    hits: List[Dict],\n","    s3_to_local: Dict[str, str],\n","    n_images: int = 4,\n","    bucket: Optional[str] = None\n",") -> List[Dict]:\n","    \"\"\"\n","    Display images in the same order we will send them to the model.\n","    Returns labeled metadata so summaries can reference S1..Sn reliably.\n","    \"\"\"\n","    labeled = []\n","    for i, h in enumerate(hits[:n_images], start=1):\n","        key = h[\"metadata\"][\"s3_key\"]\n","        path = s3_to_local.get(key)\n","        if not path:\n","            continue\n","\n","        label = f\"S{i}\"\n","        filename = h[\"metadata\"].get(\"filename\", Path(path).name)\n","        dist = h[\"distance\"]\n","\n","        labeled.append({\n","            \"label\": label,\n","            \"path\": path,\n","            \"filename\": filename,\n","            \"s3_key\": key,\n","            \"distance\": dist,\n","            \"s3_uri\": s3_key_to_display(bucket, key)\n","        })\n","\n","        img = Image.open(path)\n","        plt.figure()\n","        plt.imshow(img)\n","        plt.axis(\"off\")\n","        plt.title(f\"{label} | {filename} | dist={dist:.4f}\\n{s3_key_to_display(bucket, key)}\")\n","        plt.show()\n","\n","    return labeled\n","\n","\n","def summarize_images_with_openai(\n","    user_query: str,\n","    labeled_images: List[Dict],\n","    model: str = \"gpt-4.1-mini\"\n",") -> str:\n","    \"\"\"\n","    Send labeled images to OpenAI Vision and request a per-image description\n","    that references labels S1..Sn.\n","    \"\"\"\n","    client = OpenAI()\n","\n","    # Build a small index table for traceability\n","    index_lines = [\n","        f\"{x['label']}: {x['filename']} | {x['s3_key']} | dist={x['distance']:.4f}\"\n","        for x in labeled_images\n","    ]\n","\n","    prompt_text = (\n","        f\"User query: {user_query}\\n\\n\"\n","        \"Retrieved images index:\\n\" + \"\\n\".join(index_lines) + \"\\n\\n\"\n","        \"Instructions:\\n\"\n","        \"1) For each image S1..Sn, describe what you see in 1–2 sentences.\\n\"\n","        \"2) For each, explain briefly why it matches (or doesn't match) the query.\\n\"\n","        \"3) End with a short overall summary.\\n\"\n","        \"CRITICAL: Use the exact labels S1, S2, ... in your output.\"\n","    )\n","\n","    # OpenAI expects a single user message with mixed content blocks\n","    content = [{\"type\": \"text\", \"text\": prompt_text}]\n","\n","    # Attach images in the same order as labels\n","    for item in labeled_images:\n","        b64 = image_to_b64(item[\"path\"])\n","        content.append({\n","            \"type\": \"image_url\",\n","            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64}\"}\n","        })\n","\n","    resp = client.chat.completions.create(\n","        model=model,\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a concise visual cataloging assistant.\"},\n","            {\"role\": \"user\", \"content\": content}\n","        ],\n","        temperature=0.2\n","    )\n","    return resp.choices[0].message.content\n"],"metadata":{"id":"kEyWsGshn9W1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# 7) RUN: Step-by-step execution\n","# ============================================================\n","\n","# 7A) List & download images from S3 (start small)\n","image_keys = list_image_keys(BUCKET, prefix=PREFIX, max_keys=SAMPLE_N)\n","print(\"Found image keys:\", len(image_keys))\n","print(\"Sample keys:\", image_keys[:5])\n","\n","local_images = download_s3_objects(BUCKET, image_keys, local_dir=LOCAL_IMAGE_DIR)\n","print(\"Downloaded local files:\", len(local_images))\n","print(\"Sample local paths:\", local_images[:3])\n","\n","# Build mapping for display later\n","s3_to_local = {k: p for k, p in zip(image_keys, local_images)}\n"],"metadata":{"id":"Kre_eclwn_pv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7B) Quick sanity test: embed one image + embed one text query\n","vec_img = titan_embed_image_safe(local_images[0], output_dim=EMBED_DIM)\n","print(\"Image embedding length:\", len(vec_img), \"| first 5:\", vec_img[:5])\n","\n","vec_txt = titan_embed_text(\"t-shirt\", output_dim=EMBED_DIM)\n","print(\"Text embedding length:\", len(vec_txt), \"| first 5:\", vec_txt[:5])\n"],"metadata":{"id":"p5jaT-qUoBuy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7C) Create/load Chroma collection and upsert embeddings\n","collection = get_chroma_collection(persist_dir=CHROMA_DIR, collection_name=CHROMA_COLLECTION)\n","\n","records = build_image_records(image_keys, local_images)\n","failed = upsert_images_to_chroma(collection, records, output_dim=EMBED_DIM, batch_size=8)\n","\n","print(\"Chroma count:\", collection.count())\n","print(\"Failed examples:\", failed[:3])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWIjeWo9oDyo","executionInfo":{"status":"ok","timestamp":1766551101991,"user_tz":480,"elapsed":33587,"user":{"displayName":"Gerardo Tapia","userId":"14624640049651212323"}},"outputId":"dece262b-7175-408b-c129-7a904a1c6aa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[DONE] stored_before=65 stored_after=65 newly_added=0 failed=0\n","Chroma count: 65\n","Failed examples: []\n"]}]},{"cell_type":"code","source":["# 7D) Query and display results\n","query = \"blue t-shirt\"\n","hits = query_images(collection, query, k=8, output_dim=EMBED_DIM)\n","\n","print(\"Top hits:\")\n","for h in hits[:5]:\n","    print(f\"- dist={h['distance']:.4f} | {h['metadata']['filename']} | {h['metadata']['s3_key']}\")\n","\n","show_hit_images(hits, s3_to_local, n=6)\n"],"metadata":{"id":"p9aDQwAooFOB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7E) OPTIONAL: OpenAI Vision summary (Generation on top of Retrieval)\n","# - Make sure OPENAI_API_KEY is set in your environment.\n","# - This will display images labeled S1..Sn and then print a summary referencing S1..Sn.\n","\n","# Example:\n","labeled = display_hit_images_labeled(hits, s3_to_local, n_images=4, bucket=BUCKET)\n","summary = summarize_images_with_openai(user_query=query, labeled_images=labeled, model=\"gpt-4.1-mini\")\n","print(summary)\n"],"metadata":{"id":"_OVsqKOloHZo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# 8) Operational utilities (optional but useful)\n","# ============================================================\n","def retry_failed_images(collection, failed_list, output_dim=EMBED_DIM, batch_size=4):\n","    \"\"\"\n","    Retry embedding only the previously failed images.\n","    Useful after you fix corrupted downloads or adjust normalization.\n","    \"\"\"\n","    # Rebuild minimal records from failures (id, path)\n","    retry_records = []\n","    for _id, path, _err in failed_list:\n","        retry_records.append({\n","            \"id\": _id,\n","            \"path\": path,\n","            \"metadata\": {\n","                \"s3_key\": _id,\n","                \"filename\": Path(path).name\n","            }\n","        })\n","    return upsert_images_to_chroma(collection, retry_records, output_dim=output_dim, batch_size=batch_size)\n","\n","\n","def show_failed_images(failed_list, n=5):\n","    \"\"\"\n","    Display a few failed images (if Pillow can open them).\n","    If they are corrupted, Pillow may fail too.\n","    \"\"\"\n","    for _id, path, err in failed_list[:n]:\n","        print(\"----\")\n","        print(\"ID:\", _id)\n","        print(\"Path:\", path)\n","        print(\"Error:\", err)\n","        try:\n","            img = Image.open(path)\n","            plt.figure()\n","            plt.imshow(img)\n","            plt.axis(\"off\")\n","            plt.title(Path(path).name)\n","            plt.show()\n","        except Exception as e:\n","            print(\"Could not display image:\", e)\n","\n","# Example usage:\n","# show_failed_images(failed, n=5)\n","# failed_retry = retry_failed_images(collection, failed)\n"],"metadata":{"id":"fFP7unogoIkV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"H9pUH0VvoKpf"},"execution_count":null,"outputs":[]}]}